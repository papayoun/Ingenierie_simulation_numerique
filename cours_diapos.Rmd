---
title: "Méthodes de Monte Carlo"
subtitle: "Introduction"
author: "Pierre Gloaguen"
date: "pierre.gloaguen@agroparistech.fr"
output:
  beamer_presentation:
    dev: cairo_pdf
    includes:
      in_header: diapos_headers.tex
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, cache = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE,
                      cache = TRUE,
                      out.width = "75%",
                      fig.align = "center")
library(tidyverse)
```


```{r theme_set, cache = FALSE}
theme_set(theme_bw() +
            theme(
              panel.border = element_rect(colour = "black",
                                          fill = rgb(0, 0, 0, 0)),
              panel.grid = element_line(linetype = 2),
              plot.background = element_rect(fill = "white"),# bg around panel
              legend.background = element_blank(),
              text = element_text(family = "LM Roman 10", size = 18, face = "bold"),
              axis.title = element_text(size = rel(1.1)),
              legend.text = element_text(size = rel(1)),
              legend.title = element_text(size = rel(1.1)),
              plot.subtitle = element_text(hjust = 0.5, size = rel(1)),
              strip.background = element_rect(fill = "lightgoldenrod1"),
              plot.title = element_text(face = "bold", size = rel(1.2), hjust = 0.5)))
```


# Introduction et motivations

## Modèlisation statistique

- Formulation probabiliste d'un problème:
    - *On suppose que les données sont les réalisations de variables aléatoires telles que $\dots$*;
- Quantification de l'incertitude:
    - *d'après le modèle posé, la probabilité que tel événement arrive est de $\dots$*;
- Recours permanent au **calcul de probabilités**;

## Exemple immédiat

- Test d'hypothèse: On veut tester une hypothèse $H_0$ contre une hypothèse $H_1$
    - *Ex: Comparaison de moyennes de deux échantillons Gaussiens*;
- On construit une statistique de test $T$;
- Décision binaire, pour un risque de première espèce $\alpha$, on définit un zone de rejet $\mathcal{R}$, telle que

$$\mathbb{P}_{H_0}\left(T \in \mathcal{R}\right) = \alpha$$

\pause

**Rappel:** 
$$\mathbb{P}_{H_0}\left(T \in \mathcal{R}\right) = \mathbb{E}_{H_0}\left[\mathbf{1}_{T\in \mathcal{R}}\right]$$

- On doit donc être capable d'évaluer une espérance (ici, la probabilité d'évènements) sous $H_0$ pour construire notre test.


## Exemple de modèle statistique: Biomasse d'une population de poisson

\begin{tabular}{cc}
\multicolumn{2}{c}{\textbf{Ce qu'on connaît}} \\ 
{\color{red} Captures $C$} & {\color{blue} Observations scientifiques $Y$}\\
\includegraphics[height = 3cm, width = 0.4\textwidth]{figures/chalutier} & \includegraphics[height = 3cm, width = 0.4\textwidth]{figures/thalassa}\\
\multicolumn{2}{c}{\textbf{Quantité d'intérêt}}\\ 
\multicolumn{2}{c}{Biomasse de poisson $X$}  \\ 
\multicolumn{2}{c}{\includegraphics[width = 0.4\textwidth]{figures/poissons}}
\end{tabular} 
\end{frame}

## Modèle probabiliste d'observation de la dynamique de population

\begin{align*}
X_{t+1} &= \left( X_t + {\color{purple}r} X_t\left(1- \frac{X_t}{\color{purple}K}\right) - {\color{red} C_t} \right) \exp(\varepsilon_{t + 1}), \text{ Biomasse cachée}\\
{\color{blue} Y_t}\vert X_t &= {\color{purple}q} X_t \exp(\nu_{t}),~ {\color{red} C_t} \text{ Observations}\\
\varepsilon_t& \overset{i.i.d}{\sim}\mathcal{N}\left(-{\color{purple} \sigma}^2 / 2, {\color{purple}\sigma}^2\right),~~\nu_t\overset{i.i.d}{\sim}\mathcal{N}\left(-{\color{purple}\sigma}_\text{obs}^2 / 2, {\color{purple} \sigma}_\text{obs}^2\right).
\end{align*}

- $X_t$: Biomasse à l'année $t$ (non observée);
- $Y_t$: Abundance observée à l'année $t$;
- $C_t$: Captures à l'année $t$;
- $K$: Capacité d'accueil du milieu (paramètre);
- $r$: Taux de croissance de la population (paramètre);
- $\sigma, \sigma_\text{obs}$: Paramètres de l'aléa;
- $q$: Détectabilité (paramètre);

## Questions classiques d'inférence

\pause
- Pour un tel modèle, étant donnée une population initiale $X_0$, quelle est la moyenne attendue du nombre de poissons au bout de 10 ans si on se fixe une quantité de captures?\pause
    - $\mathbb{E}\left[X_{10} \vert X_0 \right]$?\pause
- Etant données des observations sur 10 années, et en supposant tous les paramètres connus, que puis je dire sur la quantité de poissons qu'il y avait durant ces 10 ans? \pause
    - $\mathbb{E}\left[X_{0:10} \vert Y_{0:10} \right]$? \pause
- Etant données des observations, que puis je dire sur la valeur des paramètres de dynamique de population?\pause
    - Inférence des paramètres:
    - Méthode des moments (nécessite un calcul d'espérance);\pause
    - Méthode du maximum de vraisemblance (nécessite ici un calcul d'espérance);\pause
    - Estimateur Bayésien: Nécessite un calcul d'espérance. \pause
    
Ces espérances n'ont, en général, pas d'expressions directes!

## Méthodes de Monte Carlo

\pause
- **But:**  Approcher des espérances (intégrales) en utilisant des simulations probabilistes; \pause
- **Idée:** La loi des grands nombres! La moyenne empirique d'une variable aléatoire va tendre, si on répète l'expérience, vers la moyenne théorique.

## Exemple

On dispose d'un dé à 6 faces et seulement de ce dé. Comment peut on essayer de savoir s'il est biaisé?

- Si on lance le dé suffisament de fois, on obtient une information;\pause
- Combien de fois faut il lancer le dé pour avoir une idée précise?\pause
- À quel point peut être confiant en notre réponse?\pause
- Encore faut il savoir lancer le dé!

## Programme du cours

- Présentation formelle des méthodes de Monte Carlo pour le calcul d'intégrales;
- Application directe en statistique classique (évaluation d'une probabilité, aide à la décision);
- Comment peut on simuler des variables aléatoires génériques (avec un ordinateur)?
- Une extension nécessaire, les Méthodes de Monte Carlo par chaîne de Markov (MCMC).


```{r definition_quantites_necessaires}
fonction_g <- function(x){
  d <- 10
  coeffs <- 0.5 * sin(1:d)
  sapply(x, function(z){
    sum(coeffs * z^(0:(d-1)))
  }) ^ 2
}
int_min <- -0.5
int_max <- 1
int_length <- int_max - int_min
true_val <- integrate(fonction_g, int_min, int_max)$value
fonction_phi <- function(x){
  int_length * fonction_g(x)
}
```


# Exemple introductif

## Exemple introductif (1)

Soit $g$ une fonction sur $\mathbb{R}$ et $a < b$ deux réels.

Supposons que l'on souhaite calculer une intégrale (finie) du type 
$$I = \int_a^b g(x) \rmd x$$


```{r plot_phi}
tibble(x = seq(-1.2, 1.2, length.out = 1001)) %>%
  mutate(g_x = fonction_g(x)) %>% # Calcul des valeurs de f
  mutate(borne_inf = 0) %>%
  rowwise() %>%
  mutate(borne_sup = ifelse(test = (x > int_min) & (x < int_max), 
                            yes = g_x, no = 0)) %>%
  ggplot() +
  aes(x = x)  +
  geom_ribbon(aes(ymin = borne_inf, ymax = borne_sup),
              fill = "lightblue") +
  geom_line(aes(y = g_x))  +
  labs(y = "g(x)")
```

## Exemple introductif (2)

$$\begin{array}{rl}
  I &= \int_a^b g(x) \rmd x\\
  \pause
  &=  \int_{\R} \overbrace{(b - a) g(x)}^{:= \varphi(x)} \frac{\mathbf{1}_{a\leq x\leq b}}{b - a}\rmd x\\
  \pause
  &= \E[\varphi(X)], \text{ où } X\sim\Unif[a,b].
  \end{array}$$
\pause

**Estimateur de Monte Carlo**

On fixe un entier $M > 0$.
On simule un échantillon $X_1,\dots, X_M$ selon $X\sim\Unif[a,b]$, on pose alors l'estimateur:
$$\hat{I}_M = \frac{1}{M}\sum_{k = 1}^M \varphi(X_k)$$
\pause

**Remarques**

- $M$ est appelé **effort de Monte Carlo**;
- On suppose pour le moment qu'on **sait simuler selon $\Unif[a,b]$**;
- L'estimateur de $I$ est une **variable aléatoire**.

## Exemple introductif (3)

```{r get_mc_estimate}
get_mc_estimate <- function(M, id){
  tibble(Estimate = runif(M, int_min, int_max) %>% fonction_phi() %>% mean(),
         id = id, M = M)
}
```

```{r monte_carlo_estimate_ex1}
c(10, 50, 100, 1000) %>% 
  purrr::map_dfr(function(my_N) 
    purrr::map_dfr(1:50, get_mc_estimate, M = my_N)) %>% 
  mutate(id = factor(id), M = factor(M)) %>% 
  ggplot(aes(x = M, y = Estimate)) +
  geom_boxplot() +
  labs(y = expression(hat(I)[M]),
       title = "Estimations Monte Carlo de I (50 réplicats)") +
  geom_hline(yintercept = true_val, 
             col = "red")
```

## Cas générique

On veut calculer une intégrale 
$$I = \int_{\R^d} \varphi(x) f(x) \rmd x$$
où $f$ est une fonction positive, telle que $\int_{\R^d} f(x)\rmd x = 1$, alors on se sert du fait que
$$
I = \E[\varphi(X)]
$$ 
où $X$ est une variable aléatoire de densité $f$.

**Estimateur de Monte Carlo**

On fixe un entier $M > 0$.
On simule un échantillon $X_1,\dots, X_N$ selon $X\sim f(\cdot)$, on pose alors l'estimateur:
$$\hat{I}_N = \frac{1}{M}\sum_{k= 1}^M \varphi(X_k)$$

## Pourquoi?

Les $\varphi(X_1),\dots,\varphi(X_N)$ sont des variables aléatoires i.i.d. d'espérance $\E[\varphi(X)]$ finie, avec $X\sim f(\cdot)$. \pause

Loi des grands nombres:
$$\frac{\varphi(X_1) + \dots + \varphi(X_N)}{M} \underset{M\rightarrow\infty}{\overset{\text{p.s.}}{\longrightarrow}} \E[\varphi(X)]$$ 

## Propriétés

**Sans biais**
$$\E[\hat{I}_N] = \frac{1}{M}\sum_{k = 1}^M \E[\varphi(X_k)] \overset{\text{id. distrib}}{=} \E[\varphi(X)] = I$$
\pause
**Variance**
Si $\mathbb{V}[\varphi(X)] < \infty$
$$\mathbb{V}[\hat{I}_N] \overset{\text{ind.}}{=} \frac{1}{M^2}\sum_{k = 1}^M \mathbb{V}[\varphi(X_k)] \overset{\text{id. distrib}}{=} \frac{1}{M} \mathbb{V}[\varphi(X)]$$
\pause
**Loi** La loi des grands nombres nous donne la loi asymptotique
$$\sqrt{M}\left(\hat{I}_N - I \right) \underset{M\rightarrow\infty}{\overset{\text{Loi}}{\longrightarrow}} \mathcal{N}(0, \mathbb{V}[\varphi(X)])$$

## Loi de l'estimateur

```{r n_replicates}
n_replicates <- 100
M_max <- 1000
```

- `r n_replicates` réplicats d'échantillons Monte Carlo de taille 
$M = `r M_max`$.

```{r plot_all_estimates}
all_estimates <- purrr::rerun(n_replicates, 
             tibble(phi_u = runif(M_max, int_min, int_max) %>% fonction_phi,
                    M = 1:M_max) %>% 
               mutate(Estimate = cumsum(phi_u) / M)
              ) %>% 
  bind_rows(.id = "Replicate")
ggplot(all_estimates, aes(x = M)) +
  geom_line(aes(y = Estimate, group = Replicate), alpha = 0.1) +
  geom_hline(yintercept = true_val, 
             col = "red") +
  scale_x_continuous(expand = c(0, 0))
```

## Intervalle de confiance

On note 
$$\sigma^2 = \mathbb{V}[\varphi(X)].$$

Ainsi, en notant $z_{\alpha}$ le quantile d'ordre $\alpha \in]0, 1[$ de la loi $\mathcal{N} (0, 1)$, si on définit l'intervalle aléatoire 
$$J_M = \left[\hat{I}_M - z_{1 - \alpha/2}\sqrt{\frac{\sigma^2}{M}}; \hat{I}_M + z_{1 - \alpha/2}\sqrt{\frac{\sigma^2}{M}}\right]$$
Alors, 
$$\mathbb{P}(J_M \ni I) \underset{M\rightarrow \infty}{\longrightarrow} 1-\alpha$$

$J_M$ est donc un intervalle de confiance asymptotique au niveau 1 - $\alpha$ pour la valeur de $I$.

## Intervalle de confiance (2)

En pratique cependant, cet intervalle n'est pas calculable car $\sigma^2$ est inconnu 

On dispose cependant d'un estimateur consistant de $\sigma^2$ donné par
$$\hat{\sigma}^2_M = \frac{1}{M}\sum_{k = 1}^M \left(\varphi(X_k) - \hat{I}_M\right)^2$$

- Dans l'expression précédente, on remplace $\sigma^2$ par son estimateur. 
- Le lemme de Slutski nous assure que les propriétés de $J_M$ restent vraies.

## Intervalle de confiance (3)

```{r plot_IC}
z_975 <- qnorm(0.975)
var_I1 <- integrate(function(x) (fonction_phi(x) -  true_val)^2 / int_length,
                    int_min, int_max)$value
bornes_asymp <- tibble(M = 1:M_max) %>% 
  mutate(borne_sup = true_val + z_975 * sqrt(var_I1 / M),
         borne_inf = true_val - z_975 * sqrt(var_I1 / M))
ggplot(all_estimates, aes(x = M)) +
  geom_ribbon(data = bornes_asymp, 
              aes(ymin = borne_inf, 
                  ymax = borne_sup),
              fill = "goldenrod1") +
geom_line(aes(y = Estimate, group = Replicate), alpha = 0.1) +
  geom_hline(yintercept = true_val, 
             col = "red") +
  scale_x_continuous(expand = c(0, 0))
```

## Comparaison avec l'intégration numérique

L'objectif présenté ici est de calculer, en dimension $d$, une intégrale:
$$\int_{\R^d} g(x) \rmd x$$
- Calcul possible par méthode numérique (méthode des cubes).\pause

- **Intégration numérique:** Pour une fonction $g$ de classe $C^s$, l'erreur est de l'ordre $\frac{1}{M^{\frac{s}{d}}}$ (où $M$ est le nombre d'évaluations de la fonction).
    - Il faut connaître la régularité de $g$!
    - L'erreur augmente avec la dimension.\pause

- **Méthodes Monte Carlo:** Pour les méthodes de Monte Carlo, l'écart type de l'erreur est de l'ordre $\frac{1}{M^{\frac{1}{2}}}$.
    - Indépendamment de la régularité de $g$!
    - Indépendamment de la dimension!
    
Ainsi, ces méthodes deviennent vite avantageuses quand $d$ est grand.

# Echantillonnage préférentiel

```{r functions_and_df_proposal}
get_phi <- function(x){
  sin(10 * x)^2 * exp(-5 * (x + 5)^2)
}
get_f <- function(x, log = FALSE){
  dnorm(x, log = log)
}
my_df_proposal <- 3
get_g <- function(x, df = my_df_proposal, log = FALSE){
  dt(x + 5, df = df, log = log) # Student distribution with finite variance
}
```

```{r function_tibble}
function_tibble <- tibble(x = seq(-8, 5, by = 0.001)) %>% 
  mutate(phi_x = get_phi(x), f_x = get_f(x), phi_f = phi_x * f_x,
         g_x = get_g(x), f_over_g = f_x / g_x) %>% 
  gather(-x, key = "Function", value = "Value", factor_key = TRUE) %>% 
  mutate(Function = factor(Function, labels = c("phi(x)", "f(x)",
                                                "phi(x)%*%f(x)",
                                                "g(x)", "f(x) %/% g(x)")))
```


## Exemple problématique:

On veut calculer
$$I = \mathbb{E}\left[\varphi(X) \right] = \int_{\mathbb{R}}\varphi(x)f(x)\text{d}x$$
où $X \sim \mathcal{N}(0, 1)$ (de densité $f(x)$) et 
$$\varphi(x) = \sin^2(x)\text{e}^{-5(x - 5)^2}$$

```{r plot_functions}
function_tibble %>% 
  filter(str_detect(Function, "g", negate = TRUE)) %>%
  ggplot(aes(x = x, y = Value)) +
  geom_line() +
  facet_wrap(.~Function, scales = "free", labeller = label_parsed)
```

## Estimateur Monte Carlo naturel

```{r get_mc_estimate_is}
get_mc_estimate <- function(n_samp){
  samples <- rnorm(n_samp)
  z_975 <- qnorm(0.975)
  tibble(index = 1:n_samp, x = samples, phi_x = get_phi(samples),
         w = 1) %>% 
    mutate(estimate = cumsum(phi_x) / index,
           variance = cumsum(phi_x^2) / index - estimate^2,
           borne_sup = estimate + z_975 * sqrt(variance / index),
           borne_inf = estimate - z_975 * sqrt(variance / index))
}
```

```{r my_mc_estimate}
set.seed(1234)
my_mc_estimate <- get_mc_estimate(5e4)
```

On tire $M = 50000$ points dans une loi $\mathcal{N}(0, 1)$, et on calcule la moyenne empirique.

## Résultats obtenus:

```{r resultats_estimation}
resultats <- read.table("resultats_estimation.csv",
                        sep = ",", header = T)
resultats %>% mutate(ecart_type = sqrt(variance_estimee / 5e4)) %>% 
  select(estimation, ecart_type) %>%
  filter(estimation < 0.1) %>% 
  gather(key = "Parametre", value = "Estimation", factor_key = TRUE) %>% 
  mutate(Parametre = factor(Parametre, labels = c("I", "sigma/M"))) %>% 
  ggplot(aes(y = Estimation)) +
  geom_boxplot() +
  scale_y_continuous(trans = "log10") +
  facet_wrap(.~Parametre, labeller = label_parsed) 
```

## Une trajectoire d'estimation

```{r plot_my_mc_estimate}
ggplot(my_mc_estimate) +
  aes(x = index, y = estimate) +
  geom_line() +
  labs(x = "M", y = expression(hat(I)[M]),
       title = "Estimation Monte Carlo Standard")
```

L'estimation est très instable.

## Estimateur Monte Carlo naturel

Estimation de la variance et intervalle de confiance asymptotique:

```{r plot_my_mc_estimate_IC}
plot_mc_estimate <- ggplot(my_mc_estimate) +
  aes(x = index, y = estimate) +
  geom_ribbon(aes(ymin = borne_inf, ymax = borne_sup),
              fill = "lightblue") + 
  geom_line() +
  labs(x = "M", y = expression(hat(I)[M]),
       title = "Estimation Monte Carlo Standard")
plot_mc_estimate
```

## Manque de chance?

```{r tree_monte_carlo_replicates}
set.seed(123)
three_mc_estimates <- rerun(3, get_mc_estimate(5e4)) %>% 
  bind_rows(.id = "Replicate")
ggplot(three_mc_estimates) +
  aes(x = index, y = estimate) +
  geom_ribbon(aes(ymin = borne_inf, ymax = borne_sup,
                  fill = Replicate), alpha = 0.5) + 
  geom_line(aes(group = Replicate, linetype = Replicate)) +
  theme(legend.position = "none") +
  labs(x = "M", y = expression(hat(I)[M]),
       title = "Estimation Monte Carlo Standard") +
  coord_cartesian(ylim = c(-1, 1) * 5e-6)
```

## Origine du problème

```{r plot_monte_carlo_samples}
function_tibble %>% 
  filter(Function == "phi(x)%*%f(x)") %>% 
  ggplot(aes(x = x, y = Value)) +
  geom_line() +
  geom_point(data = my_mc_estimate,
             mapping = aes(y = 0), col = "blue",
             size = 0.5)
```

On échantillonne loin des régions importantes!

## Echantillonnage préférentiel

On cherche à estimer une intégrale du type:
$$I = \mathbb{E}_f[\varphi(X)] = \int_{\mathcal{D}_f} \varphi(x) f(x)\rmd x $$
où $\mathcal{D}_f \subset \R^d$, et $f$ est une densité de probabilité sur $\mathcal{D}_f$ (donc $f(x) = 0$ pour $x \not\in \mathcal{D}_f$) et $X$ une v.a. de loi $f$. \pause

Soit $g$ une densité de probabilité sur $\mathcal{D}_g \supseteq \mathcal{D}_f$ 
telle que $x\in \mathcal{D}_f \Rightarrow g(x) > 0$ et $Y$ une variable aléatoire de loi $g$, alors:
\begin{align*}
I &=  \int_{\mathcal{D}_f} \varphi(x) \frac{f(x)}{g(x)}g(x)\rmd x &\\ \pause
&= \int_{\mathcal{D}_g}  \varphi(x) \frac{f(x)}{g(x)}g(x)\rmd x & \text{ as } x \not\in \mathcal{D}_f \Rightarrow f(x) = 0\\ \pause
&= \mathbb{E}_g\left[\varphi(Y) \frac{f(Y)}{g(Y)}\right]&
\end{align*}

## Echantillonnage preferentiel

Comme estimateur de $I$, on peut ainsi proposer l'estimateur:
$$\hat{I}^{IS}_M = \frac{1}{M}\sum_{k = 1}^M \frac{f(Y_k)}{g(Y_k)} \varphi(Y_k)  = \frac{1}{M}\sum_{k = 1}^M W(Y_k)\varphi(Y_k)$$
où $Y_1,\dots,Y_M$ est un échantillon i.i.d. de variables aléatoires sur $\R^d$ de densité $g$.\pause

**Remarques:**

- Comme $Y_k \sim g$, $g(Y_k)$ est p.s. $\neq 0$\pause
- La variable aléatoire $W(Y_k) = \frac{f(Y_k)}{g(Y_k)}$ est appelée **poids d'importance** de $Y_k$. \pause
- Quand $f = g$, on a l'estimateur MC standard (et chaque poids vaut 1).

## Quel intérêt?

On peut choisir $g$ afin d'échantillonner dans les zones d'importance! 

**Biais:** 
\begin{align*}
\E_g[\hat{I}^{IS}_M]&=\frac{1}{M}\sum_{k=1}^M\int_{\mathcal{D}_g}  \frac{f(x)}{g(x)}\varphi(x) g(x) \rmd x\\
&=\int_{\mathcal{D}_f}  f(x)\varphi(x) \rmd x = I
\end{align*}
Donc, cet estimateur reste sans biais\pause

**Variance:**
$$\V_g[\hat{I}^{IS}_1] = \left[\E_g[\left(W(Y)\varphi(Y)\right)^2]  - I^2\right] = \int_{\mathcal{D}_g}\frac{\left(\varphi(y)f(y) - Ig(y)\right)^2}{g(y)}\rmd y$$

- La variance peut être très réduite si $g(y) \overset{\approx}{\propto} \varphi(y)f(y)$!
- Ceci peut guider le choix de $g$!

## Exemple

$g(x)$ est la densité d'une loi de Student $\mathcal{T}(`r my_df_proposal`)$ centrée en -5.

```{r get_is_estimate}
get_is_estimate <- function(n_samp, df = my_df_proposal){
  samples <- rt(n_samp, df = df) - 5
  weights <- exp(get_f(samples, log = TRUE) - get_g(samples, df = df, log = TRUE))
  z_975 <- qnorm(0.975)
  tibble(index = 1:n_samp, x = samples, phi_x = get_phi(samples),
         w = weights) %>% 
    mutate(estimate = cumsum(phi_x * weights) / index,
           variance = cumsum((phi_x * weights)^2) / index - estimate^2,
           borne_sup = estimate + z_975 * sqrt(variance / index),
           borne_inf = estimate - z_975 * sqrt(variance / index))
}
```

```{r main_plot_is}
main_plot_is <- function_tibble %>% 
  filter(Function %in% c("phi(x)", "g(x)")) %>% 
  ggplot(aes(x = x, y = Value)) +
  scale_color_manual(labels = c(expression(phi(x), "g(x)")),
                     values = c("blue", "red")) +
  geom_line(aes(color = Function)) 
main_plot_is
```

## Exemple

```{r is_estimate_example}
set.seed(123)
M_example <- 30
is_estimate_example <- get_is_estimate(M_example) 
```

```{r plot_is_estimate_example}
main_plot_is +
  geom_point(data = is_estimate_example, aes(y = 0, size = w),
             color = "red") +
  guides(size = "none") +
  labs(title = paste0(M_example, " échantillons pondérés d'une Student(",
                      my_df_proposal, ") centrée en  -5"))
```

## Estimation de $I$ (et IC asymptotique)

```{r plot_is_estimate}
set.seed(1234)
plot_is_estimate <- get_is_estimate(5e4) %>%  
  ggplot() +
  aes(x = index, y = estimate) +
  geom_ribbon(aes(ymin = borne_inf, ymax = borne_sup),
              fill = "coral2") +
  geom_line() + 
  labs(x = "M", y = expression(hat(IS)[M]^IS),
       title = "Estimation par échantillonnage préférentiel")
plot_is_estimate
```

## Comparaison sur cet exemple

```{r compare_is_mc}
set.seed(1)
mutate(get_mc_estimate(1e6), Type = "Standard") %>% 
  bind_rows(mutate(get_is_estimate(1e6), Type = "Ech. Pref")) %>% 
  slice(seq(1, 2e6, by = 100)) %>% 
  ggplot() +
  aes(x = index, y = estimate) +
  geom_ribbon(aes(ymin = borne_inf, ymax = borne_sup, fill = Type),
              alpha = 0.5) +
  geom_line(aes(color = Type)) + 
  labs(x = "M", y = expression(hat(I)),
       title = "Estimation de I")
```

## Echantillonnage préférentiel

### Avantages

- Très utile pour l'estimation de quantités petites (probabilités d'évènements rares). \pause
- Peut amener à une forte réduction de variance \pause
- Peut aussi être utiliser quand on ne sait pas simuler selon $f$! \pause

### Attention!

- Nécessite le choix de $g$! Pas toujours évident (notamment en grande dimension)!\pause
- Un mauvais $g$ peut amener à un estimateur de variance infinie! (voir TD).
- Un bon choix de $g$ est souvent "problème dépendant", (ne conviendra que pour $\E[\varphi(X)]$ pour un $\varphi$ spécifique).

# Echantillonnage préférentiel normalisé

## Problématique

Objectif, calculer:
$$I = \mathbb{E}[\varphi(X)] = \int_{\mathcal{D}_f} \varphi(x) f(x) d(x)$$

Supposons que $f$ ne soit connue qu'à une constante près:
$$f(x) = \frac{\overbrace{f^{(u)}(x)}^{\text{Connu}}}{\underbrace{\int_{\mathcal{D}_f} f^{(u)}(z)\rmd z}_\text{Inconnu}}.$$

- Pour une densité de proposition $g$, on ne peut plus calculer le poids d'importance!\pause

- Ce cas est en pratique très fréquent en inférence bayésienne!

## Echantillonnage préférentiel normalisé

Si on dispose d'une loi de proposition $g$ et de $Y_1,\dots, Y_M$ tirés indépendemment selon $g$,
alors l'estimateur:
$$\hat{I}^{IS,u}_M = \sum_{k = 1}^M  \frac{f^{(u)}(Y_k)/g(Y_k)}{\sum_{\ell = 1}^M f^{(u)}(Y_\ell) / g(Y_\ell)}\varphi(Y_k)$$
est un estimateur consistant (convergence en proba.) de $I$.\pause

- Estimateur biaisé pour $M$ petit;
- Peut amener à un estimateur de variance plus faible que l'échantillonnage préférentiel classique.

# Générateurs pseudo aléatoires


## Générateurs pseudo aléatoires

- Comment simuler une loi uniforme continue avec un ordinateur?
    - Générateurs pseudo aléatoires;
- Comment simuler des lois génériques à partir de lois uniformes?
    - Méthode d'inversion;
- Comment simuler des lois non classique à partir de lois simulables?
    - Méthode d'acceptation rejet.


## Simulation de variables aléatoires par ordinateur

**Objectif: ** Simuler une suite de variables aléatoires $X_1, \dots, X_M$ telles qu'elles soient:

  - Distribuées selon une même loi donnée:
  - Mutuellement indépendantes.\pause

- Une telle simulation est faite selon un algorithme **déterministe**;\pause
- À l'heure actuelle, un ordinateur ne peut que **mimer l'aléa**;\pause
- **Mimer l'aléa:** Pour un échantillon de loi donnée:
    - Passer les tests statistiques usuels d'adéquations (test du $\chi^2$, test de Kolmogorov Smirnoff);
    - Passer les tests d'indépendances usuels (test du $\chi^2$, test de corrélation linéaire, ...).
    
## Générateur de la loi uniforme $\mathcal{U}[0, 1]$

- En pratique, la loi "atomique" est la loi $\mathcal{U}[0, 1]$;\pause
- Si on sait simuler selon cette loi, par différentes méthodes on pourra se ramener aux autres.\pause
- Nécessité d'un algorithme permettant de mimer un échantillon i.i.d. de loi $\mathcal{U}[0, 1]$.

## Algorithme de congruence linéaire

Algorithme basé sur 4 données initiales choisies par l'utilisateur:
\begin{itemize}
\item Un entier $m > 0$, appelé \textit{module};
\item Un entier $0 < a < m$  appelé \textit{multiplicateur};
\item Un entier $0 \leq c < m$ appelé \textit{incrément};
\item Un entier $0 \leq x_0 < m$ appelé \textit{graine}.
\end{itemize}\pause
On créera alors une suite de nombres $x_1,\dots x_n$ en utilisant la relation de récurrence
\begin{align*}
x_k = (a x_{k-1} + c) \text{ modulo }m. 
\end{align*}\pause
On définit enfin les nombres $u_1,\dots,u_n$  dans l'intervalle $[0, 1]$: 
$$u_k = \frac{x_k}{m},~1\leq k \leq n.$$

## Algorithme de congruence linéaire

```{r mon_algo, echo = TRUE, eval = FALSE}
mon_runif <- function(n , a, m, c, x0){
  echantillon <- rep(NA, n + 1)
  # %% est l'opérateur modulo
  x_vals[1] <- (x0 %% m) # Initialisation
  for(k in 2:(n + 1)){ # Iteration 
    x_vals[k] <- (a * x_vals[k - 1] + c) %% m 
  }
  u_vals <- x_vals / m # Mise entre 0 et 1
  return(u_vals[-1])
  # On ne retourne pas la graine
}
```

## Choix de a, m, c, x0

- À $x_0$ fixé, la séquence obtenue est **toujours la même**. 
    - En pratique, elle n'est pas demandée à l'utilisateur, mais obtenue en interne. 
    - Exemple: nombre de millisecondes (modulo $m$) écoulé depuis le 1er Janvier 1970.\pause
- Doit couvrir "tout" [0, 1]:
    - $\Rightarrow$ $m$ grand; \pause
- La suite est nécessairement **périodique**!
    - On veut une période "invisible" (mimer l'indépendance);
    - $\Rightarrow$ $a$ grand **et** relativement premier à $m$.\pause
- Considération algorithmiques sur le modulo. 
- Voir références poly.

## Choix important (distribution)

```{r mon_runif}
mon_runif <- function(n , a, m, c, x0){
  # %% est l'opérateur modulo
  x0 <- x0 %% m# On s'assure que x0 est bien plus petit que m
  xs <- rep(x0, n + 1)# Suite des xs, on ne gardera que les n derniers
  for(k in 2:(n+1)){
    xs[k] <- (a * xs[k - 1] + c) %% m 
  }
  us <- xs / m # Mise entre 0 et 1
  # Retour sous forme de data.frame, pratique pour les ggplot
  tibble(n = 1:n, valeur = us[-1])# On ne retourne pas x0 / m
}
```

3 jeux de paramètres (voir TD), donnant 3 suites de 10000 valeurs entre 0 et 1

```{r exemples}
parametres <- list(a = list(41358, 3, 101),# On stocke les différents paramètres
                   m = list(2^31 - 1, 2^31 - 1, 2311))# dans une liste
# On peut faire plusieurs traitements sans boucle dans R
resultats <- purrr::pmap_dfr(parametres, # On applique à la liste des paramètres
                             mon_runif, # La fonction
                             n = 10000, c = 0, x0 = 17, # Les paramètres manquants
                             .id = "numero")# On souhaite une colonne
```

```{r plot_histogrammes_empiriques, out.width="70%"}
ggplot(resultats) +
  aes(x = valeur) +
  geom_histogram(aes(y = ..density..),
                 fill = "lightblue", color = "blue", 
                 breaks = seq(0, 1, length.out = 21)) +
  facet_wrap(.~numero) +
  labs(y = "Densité empirique", x = "U")
```

Au risque 5%, avec un test de Kolmogorov-Smirnoff, on rejette 

- $H_0:$ Echantillon de loi uniforme $\mathcal{U}[0, 1]$

seulement pour l'échantillon 3.

## Choix important (indépendance)

On regarde, pour les 3 échantillons, la valeur de $u_k$ en fonction de celle de $u_{k-1}$:
```{r plot_autocorrelations_empiriques, warning = FALSE}
ggplot(resultats) +
  aes(x = valeur, y = lead(valeur)) +
  geom_point(size = 0.5) +
  facet_wrap(.~numero) +
  labs(y = expression(u[k]), x = expression(u[k - 1]))
```

Il y a une forte autocorrélation empirique dans les deux derniers échantillons! 

## Loi uniforme générique

- Si on sait simuler $U\sim \mathcal{U}[0, 1]$, alors, pour $a < b \in \mathbb{R}$ 
$$(b - a) U + a \sim \mathcal{U}[a, b]$$
- Dans `R`, la simulation d'une loi uniforme est faite avec `runif`;
- Dans la suite: on suppose qu'on sait simuler selon $\mathcal{U}[0, 1]$.

# Méthode d'inversion

## Rappel: Fonction de répartition

Soit $X$ une variable aléatoire à valeurs réelles. Pour tout réel $x$, on appelle fonction de répartition de $X$ la fonction $F_X$:
\begin{align*}
\begin{array}{ccl}
\R &\mapsto& [0, 1]\\
x &\mapsto& F_X(x) = \mathbb{P}(X \leq x)
\end{array}
\end{align*}\pause
Une fonction de répartition $F_X$ est caractérisée par les propriétés suivantes:
\begin{enumerate}
\item $F_X$ est partout continue à droite, i.e. pour tout $x\in \R$: $$\lim_{h \underset{>0}{\rightarrow} 0} F(x + h) = F(x)$$
\item $F_X$ est croissante.
\item $\lim_{x \rightarrow -\infty}F_X(x) = 0, \lim_{x \rightarrow +\infty}F_X(x) = 1$
\end{enumerate}
Ainsi, toute fonction $F$ sur $\R$ satisfaisant ces conditions est une fonction de répartition.

## Exemple: Fonction de répartition

\begin{figure}
\centering
\includegraphics[height = 0.7\textheight]{figures/fonction_repartition}
\caption{\label{fig:fonc:rep} Exemples de fonction de répartition pour une variable aléatoire discrète (haut), continue (centre) ou avec atome (bas). Source \textit{Wikipedia}.}
\end{figure}

## Inverse généralisée de $F$ (fonction quantile)

Soit $F$ une fonction de répartition, on appelle inverse généralisée de $F$, notée, $F^{-1}$ la fonction:
\begin{align*}
\begin{array}{ccl}
]0, 1[&\mapsto& \R\\
u &\mapsto& F^{-1}(u) = \inf\left\lbrace z \in \R \text{ tel que } F(z) \geq u \right\rbrace
\end{array}
\end{align*}
Pour une variable aléatoire $X$, la fonction $F_X^{-1}$ est également appelée *fonction quantile* de la variable aléatoire $X$.
On convient que $F_X^{-1}(0)$ et $F_X^{-1}(1)$ sont  la plus petite et la plus grande des valeurs du support de $X$ (éventuellement infinies).

## Inverse généralisée

**Remarque:** Dans le cas d'une fonction de répartition $F$ continue et strictement croissante sur $\R$, la fonction $F^{-1}$ est simplement l'inverse de $F$.

## Méthode d'inversion

Supposon qu'on ait une variable aléatoire $X$ dont on connait la  fonction de répartiion $F$, comment simuler $X$?\pause

*Exemple:* $X\sim \mathcal{E}xp(\lambda)$:

- **Densité:** $f(x) = \lambda\text{e}^{-\lambda x}\mathbf{1}_{x \geq 0}$
\pause
- **Fonction de répartition: ** $F(x) = \int_{-\infty}^x f(z)\text{d}z = (1 - \text{e}^{-\lambda x})\mathbf{1}_{x \geq 0}$
\pause
- **Inverse généralisée: ** $0<u<1,~F^{-1}(u) = -\frac{\ln (1-u)}{\lambda}$
\pause

**Méthode d'inversion** Soit $F$ une fonction de répartition. Soit $F^{-1}$ son inverse généralisée. Soit $U$ une variable aléatoire de loi uniforme sur $[0, 1]$, alors la variable aléatoire $$X := F^{-1}(U)$$ admet $F$ comme fonction de répartition.

## Exemple de méthode d'inversion

$$F^{-1}(u) = -\frac{\ln (1-u)}{\lambda}$$

```{r mon_rexp, echo = TRUE}
mon_rexp <- function(n, lambda){
  # On simule selon une loi uniforme
  us <- runif(n) # Echantillon IID U[0,1]
  # On applique la fonction quantile a l'échantillon
  - log(1 - us) / lambda
}
```

## Exemple de méthode d'inversion

$$F^{-1}(u) = -\frac{\ln (1-u)}{\lambda}$$

```{r plot_mon_rexp, cache = TRUE}
tibble(x = mon_rexp(1e4, 1)) %>% 
  ggplot(aes(x = x)) +
  geom_histogram(aes(y = ..density..),
                 color = "blue", fill = "lightblue",
                 breaks = seq(0, 12, by = 0.5)) +
  stat_function(fun = dexp, color = "red") +
  labs(x = "x", y = "Densité empirique",
       title = expression("Echantillon de taille 10000, "~lambda==1))
```

## Preuve de la méthode d'inversion

On veut montrer que, pour tout $x\in \R$, si $U\sim \mathcal{U}[0,1]$, alors
\begin{align}
\mathbb{P}(F^{-1}(U) \leq x) &= F(x).\nonumber \pause
\intertext{Montrons tout d'abord que, pour tout $u \in ]0, 1[$}
\forall x \in \R,F^{-1}(u) \leq x &\Leftrightarrow u \leq F(x). \label{eq:lemma:inv}
\end{align}
\pause
En effet, si on y parvient, il restera à conclure en se servant de la définition d'une loi uniforme:
$$\mathbb{P}(F^{-1}(U) \leq x) \overset{\text{par \eqref{eq:lemma:inv}}}{=} \mathbb{P}(U \leq F(x))  \overset{\text{car }U\sim\mathcal{U}[0,1]}= F(x).$$

## Preuve de la méthode d'inversion

Montrons d'abord que, pour tout $u \in ]0, 1[$
\begin{align*}
\forall x \in \R,F^{-1}(u) \leq x &\Rightarrow u \leq F(x).
\end{align*}
\pause
\begin{itemize}
\item[$\Rightarrow$] Soient $u \in ]0, 1[$ et $x\in \R$ tels que $F^{-1}(u) \leq x$. \newline
Par croissance de $F$, on a donc:
\begin{align*}
F\left(F^{-1}(u)\right) &\leq F(x)
\intertext{Or, en se souvenant que par définition}
F^{-1}(u) &= \inf\left\lbrace z \in \R \text{ tel que } F(z) \geq u \right\rbrace,
\intertext{on a donc directement}
u&\leq  F\left(F^{-1}(u)\right) \leq F(x)
\end{align*}
\end{itemize}

## Preuve de la méthode d'inversion

Montrons maintenant, pour tout $u \in ]0, 1[$
\begin{align*}
\forall x \in \R,F^{-1}(u) \leq x &\Leftarrow u \leq F(x).
\end{align*}\pause
\begin{itemize}
\item[$\Leftarrow$] Soient $u \in ]0, 1[$ et $x\in \R$ tels que $u \leq F(x)$.\newline
Ainsi, $x\in\left\lbrace z \in \R \text{ tel que } F(z) \geq u \right\rbrace$, donc $F^{-1}(u) \leq x$.
\end{itemize}

## Intérêt de la méthode d'inversion

Ainsi, pour une variable aléatoire à valeurs dans $\mathbb{R}$, on sait simuler si on connaît l'inverse généralisée de sa fonction de répartition.

# Méthode d'acceptation rejet

## Exemple motivant

```{r get_f_density}
get_f_density <- function(x){
  sqrt(2 / pi) * exp(-x^2 * 0.5)
}
```

```{r get_g_density}
get_g_density <- function(x){
  dexp(x, 1)
}
M <- sqrt(2 / pi) * exp(0.5)
get_ratio <- function(x){
  get_f_density(x) / (M * get_g_density(x))
}
```

On veut simuler selon la densité $f(x) = \sqrt{\frac{2}{\pi}}\text{e}^{-\frac{x^2}{2}}\mathbf{1}_{x \geq 0}$.

La fonction quantile n'a pas d'expression analytique. La méthode d'inversion ne peut être appliquée.\pause

**Principe de la méthode d'acceptation rejet:** 

- Simuler des *candidats* selon une autre loi qu'on sait simuler. \pause 
- Choisir parmi les candidats grâce à la loi uniforme.

## Méthode d'acceptation rejet (proposition)

Soit $f$ et $g$ deux densités sur $\R^d$.
On suppose qu'il existe une constante $M$ telle que 
$$\forall x \in \R^d~~f(x) \leq M g(x)$$
On note 
$$0 \leq r(x) := \frac{f(x)}{Mg(x)} \leq 1.$$
\pause
-Soient $(Y_n)_{n\geq 1}$ une suite de variables aléatoires i.i.d. de densité $g$ et $(U_n)_{n\geq 1}$ une suite de variables aléatoires i.i.d. de loi uniforme sur $[0, 1]$. 
\pause
- On note $T$ la variable aléatoire (à valeurs dans $\mathbb{N}^*$):
$$T = \inf\left\lbrace n, \text{ tel que } U_n \leq r(Y_n)\right\rbrace.$$.
\pause
- Alors, la variable aléatoire $X := Y_T$ ($T$-ième valeur de la suite  $(Y_n)_{n\geq 1}$) a pour densité $f$.

## Méthode d'acceptation rejet (algorithme)

On veut tirer un échantillon $X$ de densité $f$. 
On ne sait simuler que selon la densité $g$.
On suppose qu'il existe une constante $M$ telle que 
$$\forall x \in \R^d~~f(x) \leq M g(x)$$

**Algorithme** `Condition <- FALSE`

- Tant que `not Condition`:
    - Tirer $Y \sim g(y)$;
    - Tirer (indépendemment) $U\sim \mathcal{U}[0, 1]$;
    - Si $$U \leq \frac{f(Y)}{Mg(Y)},$$ alors on pose `Condition <- TRUE` et $X = Y$
    - Sinon `Condition <- FALSE`

En sortie, $X\sim f(x)$.

## Méthode d'acceptation rejet: Exemple

- On veut simuler selon $f(x) = \sqrt{\frac{2}{\pi}}\text{e}^{-\frac{x^2}{2}}\mathbf{1}_{x \geq 0}$
- On considère $g(x) = \text{e}^{-x}\mathbf{1}_{x \geq 0}$ (densité d'un $\mathcal{E}xp(\lambda = 1) )$  
- On montre que 
$$\forall x \in \R,~f(x) \leq \overbrace{\sqrt{\frac{2}{\pi}} \text{e}^{\frac{1}{2}}}^Mg(x)$$

```{r plot_function, out.width = "60%"}
functions_tibble <- tibble(x = seq(0, 8, length.out = 1001)) %>% 
  mutate(f = get_f_density(x),
         g = get_g_density(x),
         Mg = M * g,
         Ratio = get_ratio(x)) %>% 
  gather(-x, key = "Fonction", value = "Valeur", factor_key = TRUE) 
functions_tibble %>% 
  ggplot(aes(x = x, y = Valeur, color = Fonction)) +
  geom_line(size = 2, alpha = 0.8) +
  labs(x = "x", y = "") +
  scale_color_manual(values = c("darkgreen", "red", "orange", "purple"))
```

## Méthode d'acceptation rejet: Exemple

```{r graphiques_points_acceptes}
get_acceptation <- function(n){
  candidates <- rexp(n)
  tibble(x = candidates,
         y = 0,
         accepte = runif(n) < get_ratio(candidates)) %>% 
    mutate(Fonction = ifelse(accepte, "f", "g"))
}
```

- On simule 10000 points selon $g$

```{r plot_simulated}
set.seed(123)
simulated_points <- get_acceptation(10000)
functions_tibble %>%
  filter(Fonction != "Mg") %>% 
  ggplot(aes(x = x)) +
  geom_line(aes(y = Valeur, color = Fonction)) +
  labs(x = "x", y = "") +
  scale_color_manual(values = c("darkgreen", "red", "purple")) +
  geom_jitter(data = simulated_points, aes(y = y), color = "red",
             size = 0.5, height = 0.01, alpha = 0.5)
```

## Méthode d'acceptation rejet: Exemple

- On simule 10000 points selon $g$
- On accepte avec une probabilité donnée par le ratio

```{r plot_simulated_accepted}
set.seed(123)
simulated_points <- get_acceptation(10000)
functions_tibble %>%
  filter(Fonction != "Mg") %>% 
  ggplot(aes(x = x)) +
  geom_line(aes(y = Valeur, color = Fonction)) +
  labs(x = "x", y = "") +
  scale_color_manual(values = c("darkgreen", "red", "purple")) +
  geom_jitter(data = simulated_points, aes(y = y, color = Fonction), 
             size = 0.5,  height = 0.01, alpha = 0.5)
```

## Méthode d'acceptation rejet: Exemple

- On simule 10000 points selon $g$
- On accepte avec une probabilité donnée par le ratio
- Les points acceptés sont i.i.d. de densité $f$.

```{r plot_simulated_plus_histo}
set.seed(123)
simulated_points <- get_acceptation(10000)
functions_tibble %>%
  filter(Fonction == "f") %>% 
  ggplot(aes(x = x)) +
  geom_line(aes(y = Valeur, color = Fonction)) +
  scale_color_manual(values = "darkgreen") +
  labs(x = "x", y = "") +
  geom_jitter(data = filter(simulated_points, accepte), 
             aes(y = y), color = "darkgreen", 
             size = 0.5,  height = 0.01, alpha = 0.5) +
  geom_histogram(data = filter(simulated_points, accepte),
                 breaks = seq(0, 8, by = 0.1),
                 aes(y = ..density..), fill = "darkgreen", 
                 alpha = 0.2,
                 color = "darkgreen", linetype = 2)
```

## Remarque

Sur l'exemple précédent, au lieu de faire un "tant que", on a simuler 10000 points et on n'a retenu que les acceptés. \pause

- Proportion empirique acceptée: `r round(mean(simulated_points$accepte), 3)`
- D'un autre côté, on a $1/M = `r round(1 / M, 3)`$ 

## Preuve de la méthode d'acceptation rejet

Preuve à connaître!

- Voir le poly de cours.
- Analogue de la preuve sera demandée en devoir.

## Code R

```{r get_one_sample, eval = FALSE, echo = TRUE}
get_one_sample <- function(){
  condition <- FALSE
  while(!condition){
    y <- simulate_g(...) # Simulation selon g
    u <- runif(1) # Uniform
    # On suppose que f, g, et M existent
    condition <- u <= f(y) / (M * g(y))
  }
  return(y)
}
```


## Loi du temps d'attente

```{r get_f_sample}
get_f_sample <- function(n_sample){
  get_one_sample <- function(id){
    condition <- FALSE
    n_essai <- 0
    while(!condition){
      n_essai <- n_essai + 1
      y <- rexp(1, 1) # Simulation selon g
      u <- runif(1) # Uniform
      # On suppose que f, g, et M existent
      condition <- (u <= get_f_density(y) / (M * get_g_density(y)))
    }
    tibble(x = y, n_essai = n_essai, id = id)
  }
  parallel::mclapply(1:n_sample, get_one_sample,
                     mc.cores = parallel::detectCores()) %>% 
    bind_rows()
}
```

```{r my_f_sample}
my_f_sample <- get_f_sample(1e4)
```

On s'arrête au premier temps tel qu'une uniforme est inférieure au ratio observée.

La loi du temps d'attente (voir preuve) est une **loi géométrique** sur $\mathbb{N}^*$ de paramètre $\frac{1}{M}$ .

```{r plot_temps_attente}
my_f_sample %>% 
  group_by(n_essai) %>% 
  summarise(Proportion = n() / 10000) %>% 
  mutate(loi_temps = dgeom(n_essai - 1, 1/M)) %>% 
  ggplot(aes(x = n_essai)) + 
  geom_col(aes(y = Proportion), width = 0.1, fill = "blue") +
  geom_point(aes(y = loi_temps), size = 2, col = "red") +
  geom_line(aes(y = loi_temps), size = 1, col = "red") +
  labs(x = "Nombre d'essais") +
  geom_text(data = tibble(n_essai = 6, y = c(0.6, 0.5), 
                          texte = c("Théorique", "Empirique")),
            aes(y = y, label = texte, color = texte),
            size = 6) +
  scale_color_manual(values = c("blue", "red")) +
  scale_x_continuous(labels = 1:8, breaks = 1:8) +
  theme(legend.position = "none")
```

## Vecteurs aléatoires

Pour simuler un vecteur aléatoire $(X, Y)$, on pourra utiliser (voir poly et TD pour des exemples):

- Conditionnement;
- Changement de variables

## Changements de variables pour densité

Soit un couple de variables aléatoires $(U,V)$ de densité $f_{U,V}(u,v)$ définie sur $E_{UV} \subset \R^2$ et un couple de variables aléatoires $(X, Y)$ à valeurs dans $E_{XY} \subset \R^2$. Supposons qu'il existe une application $\phi$, $C^1$, inversible, et d'inverse $C^1$, tel que $(X, Y) = \phi(U, V)$, alors la densité jointe de $(X, Y)$ est donnée par:
$$f_{X,Y}(x, y) = f_{U,V}(\phi^{-1}(x, y))\vert\det J_{\phi^{-1}}(x, y)\vert  $$
où $J_\phi$ désigne la matrice jacobienne d'une application $\phi(u, v)$:
$$J_\phi(u, v) = \begin{pmatrix}
\frac{\delta \phi_1}{\delta u}(u, v) & \frac{\delta \phi_1}{\delta v}(u, v)\\
\frac{\delta \phi_2}{\delta u}(u, v) & \frac{\delta \phi_2}{\delta v}(u, v)
\end{pmatrix}$$